{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Nural Network From Scratch**"
      ],
      "metadata": {
        "id": "9mX1gCwjhPN0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a5DBfyBXczxf"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.datasets import mnist\n",
        "from sklearn.metrics import confusion_matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Loading"
      ],
      "metadata": {
        "id": "IxktZuYVhbIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "\n",
        "data = mnist.load_data()\n",
        "(X_train, y_train), (X_test, y_test) = data\n",
        "\n",
        "train_shape = X_train.shape\n",
        "test_shape = X_test.shape\n",
        "label_train_shape = y_train.shape\n",
        "label_test_shape = y_test.shape\n",
        "\n",
        "print(\"Shape of training images:\", train_shape)\n",
        "print(\"Shape of training labels:\", label_train_shape)\n",
        "print(\"Shape of test images:\", test_shape)\n",
        "print(\"Shape of test labels:\", label_test_shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_LkoFA1c256",
        "outputId": "e8d983c5-d723-4928-c68b-1cbd0fa0447a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of training images: (60000, 28, 28)\n",
            "Shape of training labels: (60000,)\n",
            "Shape of test images: (10000, 28, 28)\n",
            "Shape of test labels: (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Normalization"
      ],
      "metadata": {
        "id": "ye78zSyGhp8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data normalization\n",
        "X_train = np.array(X_train, dtype=np.float32) / 255\n",
        "X_test = np.array(X_test, dtype=np.float32) / 255\n",
        "\n",
        "# Flatten the training and test data\n",
        "X_train_flattened = X_train.reshape(len(X_train), -1)\n",
        "X_test_flattened = X_test.reshape(len(X_test), -1)\n",
        "\n",
        "print(f'Flattened training data shape: {X_train_flattened.shape}')\n",
        "print(f'Flattened test data shape: {X_test_flattened.shape}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mBiSR7Lc49y",
        "outputId": "faf98670-d2d5-4892-f04e-29014fe8c29c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flattened training data shape: (60000, 784)\n",
            "Flattened test data shape: (10000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-Hot Encoding"
      ],
      "metadata": {
        "id": "Rp38kDukhuQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding for labels\n",
        "num_classes = 10\n",
        "one_hot_encoded = np.zeros((len(y_train), num_classes))\n",
        "\n",
        "for i in range(len(y_train)):\n",
        "    one_hot_encoded[i, y_train[i]] = 1"
      ],
      "metadata": {
        "id": "gI5ilJcmc5VZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReLU Activation Function"
      ],
      "metadata": {
        "id": "I-jf1Zz7h1k-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLU activation function\n",
        "def relu_activation(x):\n",
        "    return np.maximum(0, x)\n"
      ],
      "metadata": {
        "id": "LYwU1oI0c8iy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax Activation Function"
      ],
      "metadata": {
        "id": "V7G40qVLh4mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Softmax activation function\n",
        "def softmax_activation(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n"
      ],
      "metadata": {
        "id": "QstcWDFoc-sS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize The Weights For Each Layer Using Random Values\n",
        "Scaled By The Initialization (sqrt(2/input_dim)) For Better Convergence in Deep Networks."
      ],
      "metadata": {
        "id": "B3cMHTneifIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights\n",
        "def initialize_params(input_dim, output_dim):\n",
        "    weights = np.random.randn(output_dim, input_dim) * np.sqrt(2.0 / input_dim)\n",
        "    biases = np.zeros((output_dim, 1))\n",
        "    return weights, biases\n",
        "\n",
        "# Define sizes for each layer\n",
        "input_layer_size = 784\n",
        "hidden_layer_size = 532\n",
        "output_layer_size = 10\n",
        "\n",
        "# Initialize weights and biases for hidden and output layers\n",
        "weights_hidden_layer, bias_hidden_layer = initialize_params(input_layer_size, hidden_layer_size)\n",
        "weights_output_layer, bias_output_layer = initialize_params(hidden_layer_size, output_layer_size)\n"
      ],
      "metadata": {
        "id": "io3TV9SjdATz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Forward propagation"
      ],
      "metadata": {
        "id": "3ijFCIPXjnRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward propagation\n",
        "def forward_pass(X):\n",
        "    # input to the hidden layer\n",
        "    hidden_layer_input = np.dot(weights_hidden_layer, X.T) + bias_hidden_layer\n",
        "    hidden_layer_output = relu_activation(hidden_layer_input)\n",
        "\n",
        "    # input to the output layer\n",
        "    output_layer_input = np.dot(weights_output_layer, hidden_layer_output) + bias_output_layer\n",
        "    output_layer_output = softmax_activation(output_layer_input)\n",
        "\n",
        "    return output_layer_output, hidden_layer_output\n"
      ],
      "metadata": {
        "id": "TWwIBuomdAec"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Back Propagation"
      ],
      "metadata": {
        "id": "UeYVrnkcjqYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Backpropagation function\n",
        "def backpropagation(X, true_labels, predicted_output, hidden_output):\n",
        "    # output layer error\n",
        "    error_output_layer = predicted_output - true_labels.T\n",
        "\n",
        "    # Gradients for the output layer weights and biases\n",
        "    gradient_weights_output = np.dot(error_output_layer, hidden_output.T) / X.shape[0]\n",
        "    gradient_bias_output = np.sum(error_output_layer, axis=1, keepdims=True) / X.shape[0]\n",
        "\n",
        "    # hidden layer error\n",
        "    error_hidden_layer = np.dot(weights_output_layer.T, error_output_layer) * (hidden_output > 0)\n",
        "\n",
        "    # Gradients for the hidden layer weights and biases\n",
        "    gradient_weights_hidden = np.dot(error_hidden_layer, X) / X.shape[0]\n",
        "    gradient_bias_hidden = np.sum(error_hidden_layer, axis=1, keepdims=True) / X.shape[0]\n",
        "\n",
        "    return gradient_weights_output, gradient_bias_output, gradient_weights_hidden, gradient_bias_hidden\n",
        "\n"
      ],
      "metadata": {
        "id": "EuQtjgzrdIHE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Training**"
      ],
      "metadata": {
        "id": "LC1z9JVrj5EN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Training function\n",
        "def train_neural_network(X_train_data, one_hot_targets, epochs, lr):\n",
        "    global weights_output_layer, bias_output_layer, weights_hidden_layer, bias_hidden_layer\n",
        "    for ep in range(epochs):\n",
        "        # Forward pass\n",
        "        predictions, hidden_layer_output = forward_pass(X_train_data)\n",
        "\n",
        "        # Compute gradients via backpropagation\n",
        "        grad_w_output, grad_b_output, grad_w_hidden, grad_b_hidden = backpropagation(\n",
        "            X_train_data, one_hot_targets, predictions, hidden_layer_output)\n",
        "\n",
        "        # Update weights and biases\n",
        "        weights_output_layer -= lr * grad_w_output\n",
        "        bias_output_layer -= lr * grad_b_output\n",
        "        weights_hidden_layer -= lr * grad_w_hidden\n",
        "        bias_hidden_layer -= lr * grad_b_hidden\n",
        "\n",
        "        # Compute loss\n",
        "        loss_value = -np.mean(np.sum(one_hot_targets * np.log(predictions.T + 1e-8), axis=1))\n",
        "        print(f\"Epoch {ep + 1}/{epochs} - Loss: {loss_value:.4f}\")\n"
      ],
      "metadata": {
        "id": "_MsVtfNfdKA8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execution"
      ],
      "metadata": {
        "id": "mh3nvlB0kH44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  training parameters\n",
        "epochs_to_run = 250\n",
        "learning_rate_value = 0.03\n",
        "train_neural_network(X_train_flattened, one_hot_encoded, epochs_to_run, learning_rate_value)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_JsPTx5dLm0",
        "outputId": "c8783552-7b1b-4c82-aa64-44d0e21cd248"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250 - Loss: 2.3899\n",
            "Epoch 2/250 - Loss: 2.3325\n",
            "Epoch 3/250 - Loss: 2.2823\n",
            "Epoch 4/250 - Loss: 2.2365\n",
            "Epoch 5/250 - Loss: 2.1937\n",
            "Epoch 6/250 - Loss: 2.1531\n",
            "Epoch 7/250 - Loss: 2.1142\n",
            "Epoch 8/250 - Loss: 2.0767\n",
            "Epoch 9/250 - Loss: 2.0405\n",
            "Epoch 10/250 - Loss: 2.0053\n",
            "Epoch 11/250 - Loss: 1.9712\n",
            "Epoch 12/250 - Loss: 1.9379\n",
            "Epoch 13/250 - Loss: 1.9055\n",
            "Epoch 14/250 - Loss: 1.8740\n",
            "Epoch 15/250 - Loss: 1.8432\n",
            "Epoch 16/250 - Loss: 1.8132\n",
            "Epoch 17/250 - Loss: 1.7839\n",
            "Epoch 18/250 - Loss: 1.7554\n",
            "Epoch 19/250 - Loss: 1.7276\n",
            "Epoch 20/250 - Loss: 1.7004\n",
            "Epoch 21/250 - Loss: 1.6739\n",
            "Epoch 22/250 - Loss: 1.6481\n",
            "Epoch 23/250 - Loss: 1.6229\n",
            "Epoch 24/250 - Loss: 1.5983\n",
            "Epoch 25/250 - Loss: 1.5743\n",
            "Epoch 26/250 - Loss: 1.5509\n",
            "Epoch 27/250 - Loss: 1.5281\n",
            "Epoch 28/250 - Loss: 1.5059\n",
            "Epoch 29/250 - Loss: 1.4842\n",
            "Epoch 30/250 - Loss: 1.4631\n",
            "Epoch 31/250 - Loss: 1.4424\n",
            "Epoch 32/250 - Loss: 1.4223\n",
            "Epoch 33/250 - Loss: 1.4028\n",
            "Epoch 34/250 - Loss: 1.3837\n",
            "Epoch 35/250 - Loss: 1.3650\n",
            "Epoch 36/250 - Loss: 1.3469\n",
            "Epoch 37/250 - Loss: 1.3292\n",
            "Epoch 38/250 - Loss: 1.3120\n",
            "Epoch 39/250 - Loss: 1.2952\n",
            "Epoch 40/250 - Loss: 1.2788\n",
            "Epoch 41/250 - Loss: 1.2628\n",
            "Epoch 42/250 - Loss: 1.2473\n",
            "Epoch 43/250 - Loss: 1.2321\n",
            "Epoch 44/250 - Loss: 1.2173\n",
            "Epoch 45/250 - Loss: 1.2029\n",
            "Epoch 46/250 - Loss: 1.1888\n",
            "Epoch 47/250 - Loss: 1.1751\n",
            "Epoch 48/250 - Loss: 1.1618\n",
            "Epoch 49/250 - Loss: 1.1487\n",
            "Epoch 50/250 - Loss: 1.1360\n",
            "Epoch 51/250 - Loss: 1.1236\n",
            "Epoch 52/250 - Loss: 1.1116\n",
            "Epoch 53/250 - Loss: 1.0998\n",
            "Epoch 54/250 - Loss: 1.0883\n",
            "Epoch 55/250 - Loss: 1.0770\n",
            "Epoch 56/250 - Loss: 1.0661\n",
            "Epoch 57/250 - Loss: 1.0554\n",
            "Epoch 58/250 - Loss: 1.0449\n",
            "Epoch 59/250 - Loss: 1.0347\n",
            "Epoch 60/250 - Loss: 1.0248\n",
            "Epoch 61/250 - Loss: 1.0151\n",
            "Epoch 62/250 - Loss: 1.0056\n",
            "Epoch 63/250 - Loss: 0.9963\n",
            "Epoch 64/250 - Loss: 0.9873\n",
            "Epoch 65/250 - Loss: 0.9784\n",
            "Epoch 66/250 - Loss: 0.9697\n",
            "Epoch 67/250 - Loss: 0.9613\n",
            "Epoch 68/250 - Loss: 0.9530\n",
            "Epoch 69/250 - Loss: 0.9449\n",
            "Epoch 70/250 - Loss: 0.9370\n",
            "Epoch 71/250 - Loss: 0.9293\n",
            "Epoch 72/250 - Loss: 0.9217\n",
            "Epoch 73/250 - Loss: 0.9143\n",
            "Epoch 74/250 - Loss: 0.9071\n",
            "Epoch 75/250 - Loss: 0.9000\n",
            "Epoch 76/250 - Loss: 0.8931\n",
            "Epoch 77/250 - Loss: 0.8863\n",
            "Epoch 78/250 - Loss: 0.8796\n",
            "Epoch 79/250 - Loss: 0.8731\n",
            "Epoch 80/250 - Loss: 0.8667\n",
            "Epoch 81/250 - Loss: 0.8605\n",
            "Epoch 82/250 - Loss: 0.8543\n",
            "Epoch 83/250 - Loss: 0.8483\n",
            "Epoch 84/250 - Loss: 0.8425\n",
            "Epoch 85/250 - Loss: 0.8367\n",
            "Epoch 86/250 - Loss: 0.8310\n",
            "Epoch 87/250 - Loss: 0.8255\n",
            "Epoch 88/250 - Loss: 0.8200\n",
            "Epoch 89/250 - Loss: 0.8147\n",
            "Epoch 90/250 - Loss: 0.8095\n",
            "Epoch 91/250 - Loss: 0.8043\n",
            "Epoch 92/250 - Loss: 0.7993\n",
            "Epoch 93/250 - Loss: 0.7944\n",
            "Epoch 94/250 - Loss: 0.7895\n",
            "Epoch 95/250 - Loss: 0.7847\n",
            "Epoch 96/250 - Loss: 0.7800\n",
            "Epoch 97/250 - Loss: 0.7754\n",
            "Epoch 98/250 - Loss: 0.7709\n",
            "Epoch 99/250 - Loss: 0.7665\n",
            "Epoch 100/250 - Loss: 0.7621\n",
            "Epoch 101/250 - Loss: 0.7579\n",
            "Epoch 102/250 - Loss: 0.7536\n",
            "Epoch 103/250 - Loss: 0.7495\n",
            "Epoch 104/250 - Loss: 0.7454\n",
            "Epoch 105/250 - Loss: 0.7414\n",
            "Epoch 106/250 - Loss: 0.7375\n",
            "Epoch 107/250 - Loss: 0.7336\n",
            "Epoch 108/250 - Loss: 0.7298\n",
            "Epoch 109/250 - Loss: 0.7261\n",
            "Epoch 110/250 - Loss: 0.7224\n",
            "Epoch 111/250 - Loss: 0.7188\n",
            "Epoch 112/250 - Loss: 0.7152\n",
            "Epoch 113/250 - Loss: 0.7117\n",
            "Epoch 114/250 - Loss: 0.7083\n",
            "Epoch 115/250 - Loss: 0.7049\n",
            "Epoch 116/250 - Loss: 0.7015\n",
            "Epoch 117/250 - Loss: 0.6982\n",
            "Epoch 118/250 - Loss: 0.6950\n",
            "Epoch 119/250 - Loss: 0.6918\n",
            "Epoch 120/250 - Loss: 0.6887\n",
            "Epoch 121/250 - Loss: 0.6856\n",
            "Epoch 122/250 - Loss: 0.6825\n",
            "Epoch 123/250 - Loss: 0.6795\n",
            "Epoch 124/250 - Loss: 0.6765\n",
            "Epoch 125/250 - Loss: 0.6736\n",
            "Epoch 126/250 - Loss: 0.6708\n",
            "Epoch 127/250 - Loss: 0.6679\n",
            "Epoch 128/250 - Loss: 0.6651\n",
            "Epoch 129/250 - Loss: 0.6624\n",
            "Epoch 130/250 - Loss: 0.6596\n",
            "Epoch 131/250 - Loss: 0.6570\n",
            "Epoch 132/250 - Loss: 0.6543\n",
            "Epoch 133/250 - Loss: 0.6517\n",
            "Epoch 134/250 - Loss: 0.6491\n",
            "Epoch 135/250 - Loss: 0.6466\n",
            "Epoch 136/250 - Loss: 0.6441\n",
            "Epoch 137/250 - Loss: 0.6416\n",
            "Epoch 138/250 - Loss: 0.6392\n",
            "Epoch 139/250 - Loss: 0.6368\n",
            "Epoch 140/250 - Loss: 0.6344\n",
            "Epoch 141/250 - Loss: 0.6321\n",
            "Epoch 142/250 - Loss: 0.6298\n",
            "Epoch 143/250 - Loss: 0.6275\n",
            "Epoch 144/250 - Loss: 0.6253\n",
            "Epoch 145/250 - Loss: 0.6230\n",
            "Epoch 146/250 - Loss: 0.6208\n",
            "Epoch 147/250 - Loss: 0.6187\n",
            "Epoch 148/250 - Loss: 0.6165\n",
            "Epoch 149/250 - Loss: 0.6144\n",
            "Epoch 150/250 - Loss: 0.6123\n",
            "Epoch 151/250 - Loss: 0.6103\n",
            "Epoch 152/250 - Loss: 0.6082\n",
            "Epoch 153/250 - Loss: 0.6062\n",
            "Epoch 154/250 - Loss: 0.6042\n",
            "Epoch 155/250 - Loss: 0.6023\n",
            "Epoch 156/250 - Loss: 0.6003\n",
            "Epoch 157/250 - Loss: 0.5984\n",
            "Epoch 158/250 - Loss: 0.5965\n",
            "Epoch 159/250 - Loss: 0.5946\n",
            "Epoch 160/250 - Loss: 0.5928\n",
            "Epoch 161/250 - Loss: 0.5910\n",
            "Epoch 162/250 - Loss: 0.5891\n",
            "Epoch 163/250 - Loss: 0.5874\n",
            "Epoch 164/250 - Loss: 0.5856\n",
            "Epoch 165/250 - Loss: 0.5838\n",
            "Epoch 166/250 - Loss: 0.5821\n",
            "Epoch 167/250 - Loss: 0.5804\n",
            "Epoch 168/250 - Loss: 0.5787\n",
            "Epoch 169/250 - Loss: 0.5770\n",
            "Epoch 170/250 - Loss: 0.5754\n",
            "Epoch 171/250 - Loss: 0.5737\n",
            "Epoch 172/250 - Loss: 0.5721\n",
            "Epoch 173/250 - Loss: 0.5705\n",
            "Epoch 174/250 - Loss: 0.5689\n",
            "Epoch 175/250 - Loss: 0.5674\n",
            "Epoch 176/250 - Loss: 0.5658\n",
            "Epoch 177/250 - Loss: 0.5643\n",
            "Epoch 178/250 - Loss: 0.5628\n",
            "Epoch 179/250 - Loss: 0.5613\n",
            "Epoch 180/250 - Loss: 0.5598\n",
            "Epoch 181/250 - Loss: 0.5583\n",
            "Epoch 182/250 - Loss: 0.5568\n",
            "Epoch 183/250 - Loss: 0.5554\n",
            "Epoch 184/250 - Loss: 0.5540\n",
            "Epoch 185/250 - Loss: 0.5526\n",
            "Epoch 186/250 - Loss: 0.5512\n",
            "Epoch 187/250 - Loss: 0.5498\n",
            "Epoch 188/250 - Loss: 0.5484\n",
            "Epoch 189/250 - Loss: 0.5470\n",
            "Epoch 190/250 - Loss: 0.5457\n",
            "Epoch 191/250 - Loss: 0.5444\n",
            "Epoch 192/250 - Loss: 0.5430\n",
            "Epoch 193/250 - Loss: 0.5417\n",
            "Epoch 194/250 - Loss: 0.5404\n",
            "Epoch 195/250 - Loss: 0.5392\n",
            "Epoch 196/250 - Loss: 0.5379\n",
            "Epoch 197/250 - Loss: 0.5366\n",
            "Epoch 198/250 - Loss: 0.5354\n",
            "Epoch 199/250 - Loss: 0.5341\n",
            "Epoch 200/250 - Loss: 0.5329\n",
            "Epoch 201/250 - Loss: 0.5317\n",
            "Epoch 202/250 - Loss: 0.5305\n",
            "Epoch 203/250 - Loss: 0.5293\n",
            "Epoch 204/250 - Loss: 0.5281\n",
            "Epoch 205/250 - Loss: 0.5270\n",
            "Epoch 206/250 - Loss: 0.5258\n",
            "Epoch 207/250 - Loss: 0.5247\n",
            "Epoch 208/250 - Loss: 0.5235\n",
            "Epoch 209/250 - Loss: 0.5224\n",
            "Epoch 210/250 - Loss: 0.5213\n",
            "Epoch 211/250 - Loss: 0.5202\n",
            "Epoch 212/250 - Loss: 0.5191\n",
            "Epoch 213/250 - Loss: 0.5180\n",
            "Epoch 214/250 - Loss: 0.5169\n",
            "Epoch 215/250 - Loss: 0.5158\n",
            "Epoch 216/250 - Loss: 0.5148\n",
            "Epoch 217/250 - Loss: 0.5137\n",
            "Epoch 218/250 - Loss: 0.5127\n",
            "Epoch 219/250 - Loss: 0.5116\n",
            "Epoch 220/250 - Loss: 0.5106\n",
            "Epoch 221/250 - Loss: 0.5096\n",
            "Epoch 222/250 - Loss: 0.5086\n",
            "Epoch 223/250 - Loss: 0.5076\n",
            "Epoch 224/250 - Loss: 0.5066\n",
            "Epoch 225/250 - Loss: 0.5056\n",
            "Epoch 226/250 - Loss: 0.5046\n",
            "Epoch 227/250 - Loss: 0.5037\n",
            "Epoch 228/250 - Loss: 0.5027\n",
            "Epoch 229/250 - Loss: 0.5018\n",
            "Epoch 230/250 - Loss: 0.5008\n",
            "Epoch 231/250 - Loss: 0.4999\n",
            "Epoch 232/250 - Loss: 0.4989\n",
            "Epoch 233/250 - Loss: 0.4980\n",
            "Epoch 234/250 - Loss: 0.4971\n",
            "Epoch 235/250 - Loss: 0.4962\n",
            "Epoch 236/250 - Loss: 0.4953\n",
            "Epoch 237/250 - Loss: 0.4944\n",
            "Epoch 238/250 - Loss: 0.4935\n",
            "Epoch 239/250 - Loss: 0.4926\n",
            "Epoch 240/250 - Loss: 0.4918\n",
            "Epoch 241/250 - Loss: 0.4909\n",
            "Epoch 242/250 - Loss: 0.4900\n",
            "Epoch 243/250 - Loss: 0.4892\n",
            "Epoch 244/250 - Loss: 0.4883\n",
            "Epoch 245/250 - Loss: 0.4875\n",
            "Epoch 246/250 - Loss: 0.4867\n",
            "Epoch 247/250 - Loss: 0.4858\n",
            "Epoch 248/250 - Loss: 0.4850\n",
            "Epoch 249/250 - Loss: 0.4842\n",
            "Epoch 250/250 - Loss: 0.4834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction function\n",
        "def predict_labels(X_test_data):\n",
        "    output_probs, _ = forward_pass(X_test_data)\n",
        "    predicted_classes = np.argmax(output_probs, axis=0)\n",
        "    return predicted_classes\n",
        "\n",
        "# Generate predictions using the test data\n",
        "test_predictions = predict_labels(X_test_flattened)\n",
        "\n",
        "# accuracy\n",
        "test_accuracy = np.sum(test_predictions == y_test) / len(y_test)\n",
        "print(f\"Accuracy on the test set: {test_accuracy:.2%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYJ6ognpdNH0",
        "outputId": "fa7c28ac-ef20-46a3-998f-f7f8fcd349a9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 88.76%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "confusion_mat = confusion_matrix(y_test, test_predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bp5_0LW6dQDO",
        "outputId": "98f11895-1088-4da2-c7cb-d94d7ffbafa0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[ 944    0    3    3    1    5   17    1    6    0]\n",
            " [   0 1103    6    4    0    2    5    1   14    0]\n",
            " [  17   15  871   19   17    0   20   25   40    8]\n",
            " [   5    1   19  883    1   36    6   18   28   13]\n",
            " [   5    6    3    0  888    2   11    1    8   58]\n",
            " [  16   12    4   51   19  713   25    9   30   13]\n",
            " [  19    3    8    1   14   19  887    1    6    0]\n",
            " [   4   27   30    3   12    1    2  907    4   38]\n",
            " [   7   13   11   34   12   29   15   13  823   17]\n",
            " [  11   10   10   10   61   12    0   30    8  857]]\n"
          ]
        }
      ]
    }
  ]
}